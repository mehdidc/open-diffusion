# use pre-trained clip b32 with new projections, use pre-trained unet, and fine-tune everything
wandb:
    api_key: null
    entity: null
    mode: offline
system:
    gradient_accumulation: 1
    batch_size: 64
    workers: 4
    use_pooled_for_conditioning: False
    image_to_image_loss_weight: 0.5
    text_to_image_loss_weight: 0.5
    train_unet: True
    train_clip: True
    clip_loss_weight: 0
    dist_backend: ${distributed.dist_backend}
    dist_url: ${distributed.dist_url}

distributed:
    dist_backend: 'nccl'
    dist_url: 'env://'

experiment:
    log_dir: ./logs
    name: "exp1"
    project: "stable-diffusion-clip"
    num_examples_to_see: 1400000000
    save_every: 5000
    requeue: True
    eval_caption_file: data/prompts/uid_caption.csv
    num_eval_images: 200

optimizer:
    name: adamw
    params:
        learning_rate: 0.0001
        beta1: 0.9
        beta2: 0.98 # changed from initial sd value for training stability
        weight_decay: 0.01
        epsilon: 0.00000001

model:
    pt_path: pretrained/stable-diffusion-2-1
    vae:
        pretrained: ${model.pt_path}
    clip:
        pretrained: ${model.pt_path}
    tokenizer:
        pretrained: ${model.pt_path}
    noise_scheduler_training:
        pretrained: ${model.pt_path}
        target: DDPMNoiseScheduler
    noise_scheduler_inference:
        pretrained: ${model.pt_path}
        target: DDIMScheduler
    unet:
        target: UNet2DConditionModel
        pretrained: ${model.pt_path}
    use_ema: True 
    mixed_precision: bf16
    gradient_checkpointing: True
    xformers: True

dataset:
    type: WebDataset
    params: 
        path: /p/scratch/ccstdl/cherti1/datacomp_1B/{0000000..0139827}.tar
        batch_size: ${system.batch_size}
        workers: ${system.workers}
        num_examples_to_see: ${experiment.num_examples_to_see}
        resolution: 224

lr_scheduler:
    scheduler: "ConstantWithWarmup"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_length: 500