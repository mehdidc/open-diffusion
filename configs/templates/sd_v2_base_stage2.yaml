wandb:
    api_key: ???
    entity: ???

system:
    gradient_accumulation: 1
    batch_size: 32
    workers: 6
    dist_backend: ${distributed.dist_backend}
    dist_url: ${distributed.dist_url}

distributed:
    dist_backend: 'nccl'
    dist_url: 'env://'

experiment:
    log_dir: ./logs
    name: "stage1-sdv2"
    project: "stable-diffusion"
    num_examples_to_see: 2000000000
    save_every: 2000
    requeue: True

optimizer:
    name: adamw
    params:
        learning_rate: 0.0001
        beta1: 0.9
        beta2: 0.98 # changed from initial sd value for training stability
        weight_decay: 0.01
        epsilon: 0.00000001

model:
    pt_path: /path/to/pretrained/pipeline
    vae:
        pretrained: ${model.pt_path}

    text_encoder:
        pretrained: ${model.pt_path}

    tokenizer:
        pretrained: ${model.pt_path}
    
    scheduler:
        pretrained: ${model.pt_path}
    
    unet:
        target: UNet2DConditionModel
        params:
            act_fn: silu
            attention_head_dim:
            - 5
            - 10
            - 20
            - 20
            block_out_channels:
            - 320
            - 640
            - 1280
            - 1280
            center_input_sample: false
            cross_attention_dim: 1024
            down_block_types:
            - CrossAttnDownBlock2D
            - CrossAttnDownBlock2D
            - CrossAttnDownBlock2D
            - DownBlock2D
            downsample_padding: 1
            dual_cross_attention: false
            flip_sin_to_cos: true
            freq_shift: 0
            in_channels: 4
            layers_per_block: 2
            mid_block_scale_factor: 1
            norm_eps: 1.0e-05
            norm_num_groups: 32
            out_channels: 4
            sample_size: 64
            up_block_types:
            - UpBlock2D
            - CrossAttnUpBlock2D
            - CrossAttnUpBlock2D
            - CrossAttnUpBlock2D
            use_linear_projection: true
        
    use_ema: True 
    mixed_precision: bf16
    gradient_checkpointing: True

dataset:
    type: WebDataset
    params: 
        path: /path/to/laion-aesthetics/shards
        batch_size: ${system.batch_size}
        workers: ${system.workers}
        num_examples_to_see: ${experiment.num_examples_to_see}
        resolution: 512

        filters:
            punsafe: "<=0.1"

            # If this is slow, re-shard the filtered webdataset
            original_width: ">=512"
            original_height: ">=512"

lr_scheduler:
    scheduler: "ConstantWithWarmup"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_length: 10000
